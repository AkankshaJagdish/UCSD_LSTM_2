{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "  DATASET_PATH =\"UCSD_Anomaly_Dataset.v1p2/UCSDped1/Train\"\n",
    "  SINGLE_TEST_PATH = \"UCSD_Anomaly_Dataset.v1p2/UCSDped1/Test/Test032\"\n",
    "  BATCH_SIZE = 4\n",
    "  EPOCHS = 3\n",
    "  MODEL_PATH = \"model_lstm.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_progress_bar(iteration, total, bar_length=50):\n",
    "    progress = (iteration / total)\n",
    "    arrow = '-' * int(round(progress * bar_length) - 1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "\n",
    "    print('\\r[{}] {}/{} ({:.2f}%)'.format(arrow + spaces, iteration, total, progress * 100), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join, isdir\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shelve\n",
    "def get_clips_by_stride(stride, frames_list, sequence_size):\n",
    "    \"\"\" For data augmenting purposes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    stride : int\n",
    "        The desired distance between two consecutive frames\n",
    "    frames_list : list\n",
    "        A list of sorted frames of shape 256 X 256\n",
    "    sequence_size: int\n",
    "        The size of the desired LSTM sequence\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of clips , 10 frames each\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    sz = len(frames_list)\n",
    "    clip = np.zeros(shape=(sequence_size, 256, 256, 1))\n",
    "    cnt = 0\n",
    "    for start in range(0, stride):\n",
    "        for i in range(start, sz, stride):\n",
    "            clip[cnt, :, :, 0] = frames_list[i]\n",
    "            cnt = cnt + 1\n",
    "            if cnt == sequence_size:\n",
    "                clips.append(np.copy(clip))\n",
    "                cnt = 0\n",
    "    return clips\n",
    "\n",
    "\n",
    "def get_training_set():\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of training sequences of shape (NUMBER_OF_SEQUENCES,SINGLE_SEQUENCE_SIZE,FRAME_WIDTH,FRAME_HEIGHT,1)\n",
    "    \"\"\"\n",
    "    #####################################\n",
    "    # cache = shelve.open(Config.CACHE_PATH)\n",
    "    # return cache[\"datasetLSTM\"]\n",
    "    #####################################\n",
    "    clips = []\n",
    "    # loop over the training folders (Train000,Train001,..)\n",
    "    for f in sorted(listdir(Config.DATASET_PATH)):\n",
    "        if isdir(join(Config.DATASET_PATH, f)):\n",
    "            all_frames = []\n",
    "            # loop over all the images in the folder (0.tif,1.tif,..,199.tif)\n",
    "            for c in sorted(listdir(join(Config.DATASET_PATH, f))):\n",
    "                if str(join(join(Config.DATASET_PATH, f), c))[-3:] == \"tif\":\n",
    "                    img = Image.open(join(join(Config.DATASET_PATH, f), c)).resize((256, 256))\n",
    "                    img = np.array(img, dtype=np.float32) / 256.0\n",
    "                    all_frames.append(img)\n",
    "            # get the 10-frames sequences from the list of images after applying data augmentation\n",
    "            for stride in range(1, 3):\n",
    "                clips.extend(get_clips_by_stride(stride=stride, frames_list=all_frames, sequence_size=10))\n",
    "    return clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(input_size + hidden_size, 4 * hidden_size, kernel_size, 1, self.padding)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        h_cur, c_cur = state\n",
    "        combined = torch.cat([x, h_cur], dim=1)\n",
    "        gates = self.conv(combined)\n",
    "        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n",
    "        ingate = torch.sigmoid(ingate)\n",
    "        forgetgate = torch.sigmoid(forgetgate)\n",
    "        cellgate = torch.tanh(cellgate)\n",
    "        outgate = torch.sigmoid(outgate)\n",
    "\n",
    "        c_next = (forgetgate * c_cur) + (ingate * cellgate)\n",
    "        h_next = outgate * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def initialize_hidden_state(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_size, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_size, height, width, device=self.conv.weight.device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size, num_layers):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self._all_layers = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_size if i == 0 else self.hidden_size\n",
    "            cell = ConvLSTMCell(input_size=cur_input_dim,\n",
    "                                hidden_size=self.hidden_size,\n",
    "                                kernel_size=self.kernel_size)\n",
    "            self._all_layers.append(cell)\n",
    "            self.add_module('cell_{}'.format(i), cell)\n",
    "\n",
    "    def forward(self, x, hidden_state=None):\n",
    "        internal_state = []\n",
    "        outputs = []\n",
    "        for step in range(x.size(1)):\n",
    "            x_t = x[:, step, :, :, :]\n",
    "            for i in range(self.num_layers):\n",
    "                # get or initialize the internal state\n",
    "                if hidden_state is None:\n",
    "                    h, c = self._all_layers[i].initialize_hidden_state(batch_size=x_t.size(0), image_size=(x_t.size(2), x_t.size(3)))\n",
    "                else:\n",
    "                    h, c = hidden_state[i]\n",
    "                h, c = self._all_layers[i](x_t, [h, c])\n",
    "                x_t = h\n",
    "                if i == (self.num_layers - 1):\n",
    "                    outputs.append(h)\n",
    "            internal_state.append([h, c])\n",
    "\n",
    "        layer_output = torch.stack(outputs, dim=1)\n",
    "        return layer_output, internal_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AnomalyDetectionModel, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder_time_distributed_conv1 = nn.Conv2d(1, 128, kernel_size=11, stride=4, padding=5)\n",
    "        self.encoder_time_distributed_conv2 = nn.Conv2d(128, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.encoder_conv_lstm1 = ConvLSTM(64, 64, kernel_size=3, num_layers=1)\n",
    "        self.encoder_conv_lstm2 = ConvLSTM(64, 32, kernel_size=3, num_layers=1)\n",
    "        self.encoder_conv_lstm3 = ConvLSTM(32, 64, kernel_size=3, num_layers=1)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_time_distributed_conv_transpose1 = nn.ConvTranspose2d(64, 64, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
    "        self.decoder_time_distributed_conv_transpose2 = nn.ConvTranspose2d(64, 128, kernel_size=11, stride=4, padding=2, output_padding=1)\n",
    "        self.decoder_time_distributed_conv3 = nn.Conv2d(128, 1, kernel_size=10, padding=3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape to handle time distributed operations\n",
    "        b, t, h, w, c = x.size()\n",
    "        x = x.view(b*t, c, h, w)\n",
    "        \n",
    "        # Encoder\n",
    "        x = self.encoder_time_distributed_conv1(x)\n",
    "        \n",
    "        x = self.encoder_time_distributed_conv2(x)\n",
    "        \n",
    "        # Reshape for LSTM layers\n",
    "        x = x.view(b, t, x.size(1), x.size(2), x.size(3))\n",
    "        \n",
    "        x, _ = self.encoder_conv_lstm1(x)\n",
    "        \n",
    "        x, _ = self.encoder_conv_lstm2(x)\n",
    "        \n",
    "        x, _ = self.encoder_conv_lstm3(x)\n",
    "\n",
    "        # Reshape for decoder conv layers\n",
    "        x = x.view(b*t, x.size(2), x.size(3), x.size(4))\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder_time_distributed_conv_transpose1(x)\n",
    "        \n",
    "        x = self.decoder_time_distributed_conv_transpose2(x)\n",
    "        \n",
    "        x = self.decoder_time_distributed_conv3(x)\n",
    "\n",
    "        x = x[:, :, :256, :256]\n",
    "\n",
    "        # Reshape back to original shape\n",
    "        x = x.view(b, t, h, w, c)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        for i, sequences in enumerate(train_loader):\n",
    "            for sequences in train_loader:\n",
    "                sequences = sequences.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Reshape sequences for 2D conv operations\n",
    "                b, t, h, w, c = sequences.size()\n",
    "                sequences_reshaped = sequences.view(b*t, c, h, w)\n",
    "                \n",
    "                outputs = model(sequences)\n",
    "                \n",
    "                # Compute loss on reshaped tensors\n",
    "                loss = criterion(outputs.view(b*t, c, h, w), sequences_reshaped)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch [{epoch+1}/{Config.EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "            simple_progress_bar(i + 1, len(train_loader))\n",
    "            print()  # Go to the next line after each epoch\n",
    "\n",
    "# Replace get_model with PyTorch version\n",
    "def get_model(reload_model=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = AnomalyDetectionModel().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if reload_model:\n",
    "        model.load_state_dict(torch.load(Config.MODEL_PATH))\n",
    "        model.eval()\n",
    "    else:\n",
    "        training_set = get_training_set()\n",
    "        training_set = torch.tensor(training_set).float()\n",
    "        train_loader = DataLoader(training_set, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "        train_model(model, train_loader, optimizer, criterion, device)\n",
    "        torch.save(model.state_dict(), Config.MODEL_PATH)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_single_test():\n",
    "    sz = 200\n",
    "    test = np.zeros(shape=(sz, 256, 256, 1))\n",
    "    cnt = 0\n",
    "    for f in sorted(listdir(Config.SINGLE_TEST_PATH)):\n",
    "        if str(join(Config.SINGLE_TEST_PATH, f))[-3:] == \"tif\":\n",
    "            img = Image.open(join(Config.SINGLE_TEST_PATH, f)).resize((256, 256))\n",
    "            img = np.array(img, dtype=np.float32) / 256.0\n",
    "            test[cnt, :, :, 0] = img\n",
    "            cnt = cnt + 1\n",
    "    return test\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Update evaluate function to use PyTorch for model prediction\n",
    "def evaluate():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(False).to(device)\n",
    "    test = get_single_test()\n",
    "    test = torch.tensor(np.array(test)).float().to(device)\n",
    "\n",
    "    sz = test.size(0) - 10 + 1\n",
    "    sequences = torch.zeros(sz, 10, 256, 256, 1)\n",
    "    for i in range(sz):\n",
    "        sequences[i] = test[i:i+10]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        reconstructed_sequences = model(sequences)\n",
    "        sequences_reconstruction_cost = torch.norm(sequences - reconstructed_sequences, dim=[2,3,4])\n",
    "\n",
    "    sa = (sequences_reconstruction_cost - torch.min(sequences_reconstruction_cost)) / torch.max(sequences_reconstruction_cost)\n",
    "    sr = 1.0 - sa\n",
    "\n",
    "    plt.plot(sr.cpu().numpy())\n",
    "    plt.ylabel('regularity score Sr(t)')\n",
    "    plt.xlabel('frame t')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akank\\AppData\\Local\\Temp\\ipykernel_26572\\4245504223.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  training_set = torch.tensor(training_set).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Loss: 0.0070\n",
      "[>                                                 ] 1/340 (0.29%)\n",
      "Epoch [1/3], Loss: 0.0042\n",
      "[>                                                 ] 2/340 (0.59%)\n",
      "Epoch [1/3], Loss: 0.0046\n",
      "[>                                                 ] 3/340 (0.88%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\akank\\OneDrive\\Desktop\\PDC_PROJECT\\UCSD_LSTM_2\\anomaly_detection_3.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Call evaluate to test the converted code\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m evaluate()\n",
      "\u001b[1;32mc:\\Users\\akank\\OneDrive\\Desktop\\PDC_PROJECT\\UCSD_LSTM_2\\anomaly_detection_3.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate\u001b[39m():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     model \u001b[39m=\u001b[39m get_model(\u001b[39mFalse\u001b[39;49;00m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     test \u001b[39m=\u001b[39m get_single_test()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     test \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39marray(test))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32mc:\\Users\\akank\\OneDrive\\Desktop\\PDC_PROJECT\\UCSD_LSTM_2\\anomaly_detection_3.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     training_set \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(training_set)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     train_loader \u001b[39m=\u001b[39m DataLoader(training_set, batch_size\u001b[39m=\u001b[39mConfig\u001b[39m.\u001b[39mBATCH_SIZE, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     train_model(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), Config\u001b[39m.\u001b[39mMODEL_PATH)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;32mc:\\Users\\akank\\OneDrive\\Desktop\\PDC_PROJECT\\UCSD_LSTM_2\\anomaly_detection_3.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Compute loss on reshaped tensors\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mview(b\u001b[39m*\u001b[39mt, c, h, w), sequences_reshaped)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/akank/OneDrive/Desktop/PDC_PROJECT/UCSD_LSTM_2/anomaly_detection_3.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mConfig\u001b[39m.\u001b[39mEPOCHS\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\akank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\akank\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Call evaluate to test the converted code\n",
    "evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
